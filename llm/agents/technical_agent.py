from typing import Dict, Any
from fastmcp import Client
from langchain_openai import AzureChatOpenAI
from langgraph.types import Command
from langgraph.graph import END
import asyncio

def technical_agent(state: Dict[str, Any]) -> Command:
    """ë³´ì¡°ì§€í‘œ MCP íˆ´ì„ í˜¸ì¶œí•˜ì—¬ ê¸°ìˆ ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½"""
    query = state["task"]

    # ğŸ”¹ LLM ì„¤ì •
    llm = AzureChatOpenAI(
        azure_deployment="gpt-4o-mini",
        azure_endpoint="https://sb-azure-openai-studio.openai.azure.com/",
        api_version="2024-10-21",
        temperature=0
    )

    # 1ï¸âƒ£ ì‚¬ìš©ì ìš”ì²­ì—ì„œ ë³´ì¡°ì§€í‘œ ì¢…ë¥˜ ì¶”ì¶œ
    reasoning = llm.invoke([
        {"role": "user", "content": f"'{query}' ë¬¸ì¥ì—ì„œ í•„ìš”í•œ ë³´ì¡°ì§€í‘œë¥¼ í•˜ë‚˜ ì¶”ì¶œí•˜ê³  "
                                    f"ì§€í‘œ ì´ë¦„ë§Œ ì˜ì–´ë¡œ ë‹µí•´ì¤˜ (ì˜ˆ: rsi, macd, bollinger, mfi, stochastic, ema, sma, wma ì¤‘ í•˜ë‚˜)."}
    ]).content.strip().lower()

    # ê¸°ë³¸ê°’ ì„¤ì •
    indicator_type = reasoning if reasoning in [
        "rsi", "macd", "bollinger", "mfi", "stochastic", "ema", "sma", "wma"
    ] else "rsi"

    # 2ï¸âƒ£ MCP í˜¸ì¶œ
    async def run_client():
        async with Client("http://127.0.0.1:8005/sse") as client:
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” DBë‚˜ APIì—ì„œ ê°€ì ¸ì˜¨ OHLC ë°ì´í„°ê°€ ì—¬ê¸°ì— ë“¤ì–´ê°
            sample_data = [
                {"Open": 70000, "High": 71000, "Low": 69000, "Close": 70500, "Volume": 100000},
                {"Open": 70500, "High": 71500, "Low": 70000, "Close": 71200, "Volume": 110000},
                {"Open": 71200, "High": 72000, "Low": 71000, "Close": 71800, "Volume": 105000},
                {"Open": 71800, "High": 72500, "Low": 71500, "Close": 72000, "Volume": 120000},
                {"Open": 72000, "High": 73000, "Low": 71800, "Close": 72800, "Volume": 130000},
                {"Open": 72800, "High": 73500, "Low": 72500, "Close": 73200, "Volume": 125000},
            ]
            return await client.call_tool("get_indicator", {
                "indicator_type": indicator_type,
                "data": sample_data
            })

    tool_result = asyncio.run(run_client())

    # 3ï¸âƒ£ LLMìœ¼ë¡œ í•´ì„/ìš”ì•½
    summary = llm.invoke([
        {"role": "user", "content": f"ë³´ì¡°ì§€í‘œ '{indicator_type}' ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ì‹œì¥ íë¦„ì„ ê°„ë‹¨íˆ í•œêµ­ì–´ë¡œ í•´ì„í•´ì¤˜.\n\n{tool_result}"}
    ]).content

    # 4ï¸âƒ£ ì‘ë‹µ êµ¬ì„±
    update = {
        "response": f"[Technical Agent]\nì§€í‘œ ì¢…ë¥˜: {indicator_type.upper()}\n\n"
                    f"[MCP ê²°ê³¼]\n{tool_result}\n\n"
                    f"[LLM í•´ì„]\n{summary}",
        "handled_by": "technical_agent",
    }

    # 5ï¸âƒ£ HITL (Human-in-the-loop) ì—¬ë¶€ì— ë”°ë¼ ê²°ì •
    if state.get("require_human"):
        return Command(update=update, goto="human_review_agent")
    else:
        return Command(update=update, goto=END)
